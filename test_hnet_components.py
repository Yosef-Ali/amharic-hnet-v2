#!/usr/bin/env python3
"""
Test H-Net Components - Morpheme Chunker and Hierarchical Processing
"""

import torch
import sys
import os
sys.path.append('src')

from models.hnet_amharic import AmharicHNet
import json

def test_morpheme_chunker():
    """Test the AmharicMorphemeChunker with real Amharic words"""
    
    print("ğŸ§ª Testing H-Net Morpheme Chunker")
    print("=" * 50)
    
    # Load the trained model
    model_path = "outputs/compact/final_model.pt"
    
    model = AmharicHNet(
        d_model=256,
        n_encoder_layers=2,
        n_decoder_layers=2,
        n_main_layers=4,
        n_heads=4,
        vocab_size=256
    )
    
    # Load trained weights
    checkpoint = torch.load(model_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"âœ… Model loaded (loss: {checkpoint.get('loss', 'unknown')})")
    
    # Test Amharic words with known morphological structure
    test_words = [
        {
            "word": "á‹­áˆáˆáŒ‹áˆ‰",  # they want
            "expected_morphemes": ["á‹­", "áˆáˆáŒ", "áŠ áˆ", "á‹"],
            "meaning": "3rd.MASC.PL-want-AUX-3rd.PL"
        },
        {
            "word": "áŠ áˆáˆ˜áŒ£á‰½áˆ",  # she did not come  
            "expected_morphemes": ["áŠ áˆ", "áˆ˜áŒ£", "á‰½", "áˆ"],
            "meaning": "NEG-come-3rd.FEM.SG-NEG"
        },
        {
            "word": "á‰ á‰¤á‰³á‰½áŠ•",   # in our house
            "expected_morphemes": ["á‰ ", "á‰¤á‰µ", "áŠ á‰½áŠ•"],
            "meaning": "PREP-house-1st.PL.POSS"
        },
        {
            "word": "á‰°áˆ›áˆªá‹á‰½",   # students
            "expected_morphemes": ["á‰°áˆ›áˆª", "á‹á‰½"],
            "meaning": "student-PL"
        },
        {
            "word": "áŠ¢á‰µá‹®áŒµá‹«á‹Š",  # Ethiopian
            "expected_morphemes": ["áŠ¢á‰µá‹®áŒµá‹«", "á‹Š"],
            "meaning": "Ethiopia-ADJ"
        }
    ]
    
    print(f"\nğŸ” Testing {len(test_words)} Amharic words with morphological structure:")
    print("-" * 60)
    
    for i, test_case in enumerate(test_words, 1):
        word = test_case["word"]
        expected = test_case["expected_morphemes"]
        meaning = test_case["meaning"]
        
        print(f"\n{i}. Testing: {word}")
        print(f"   Expected morphemes: {' + '.join(expected)}")
        print(f"   Meaning: {meaning}")
        
        # Convert to bytes and embed
        word_bytes = word.encode('utf-8')
        input_ids = torch.tensor([[b for b in word_bytes]], dtype=torch.long)
        
        try:
            # Get embeddings
            embeddings = model.byte_embedding(input_ids)
            
            # Test morpheme chunker
            with torch.no_grad():
                boundary_probs = model.chunker(embeddings)
            
            # Analyze boundaries
            probs = boundary_probs[0].tolist()
            boundaries = [i for i, p in enumerate(probs) if p > 0.5]
            
            print(f"   ğŸ“Š Boundary probabilities: {[f'{p:.3f}' for p in probs]}")
            print(f"   ğŸ”„ Detected boundaries at positions: {boundaries}")
            print(f"   ğŸ“ˆ Number of predicted morphemes: {len(boundaries)}")
            print(f"   ğŸ“ˆ Expected number of morphemes: {len(expected)}")
            
            # Check if boundary count is reasonable
            boundary_accuracy = "âœ… Good" if abs(len(boundaries) - len(expected)) <= 1 else "âŒ Poor"
            print(f"   ğŸ¯ Boundary detection: {boundary_accuracy}")
            
            # Test if the chunker recognizes morphological patterns
            verb_prefixes = model.chunker.verb_prefixes
            verb_suffixes = model.chunker.verb_suffixes
            noun_prefixes = model.chunker.noun_prefixes
            noun_suffixes = model.chunker.noun_suffixes
            
            found_patterns = []
            for prefix in verb_prefixes + noun_prefixes:
                if word.startswith(prefix):
                    found_patterns.append(f"prefix:{prefix}")
            
            for suffix in verb_suffixes + noun_suffixes:
                if word.endswith(suffix):
                    found_patterns.append(f"suffix:{suffix}")
            
            if found_patterns:
                print(f"   ğŸ§  Recognized patterns: {', '.join(found_patterns)}")
            else:
                print(f"   âš ï¸ No known morphological patterns detected")
                
        except Exception as e:
            print(f"   âŒ Error processing {word}: {e}")
    
    print(f"\n" + "=" * 60)
    return model

def test_hierarchical_processing(model):
    """Test the hierarchical processing capabilities"""
    
    print(f"\nğŸ—ï¸ Testing Hierarchical Processing")
    print("=" * 50)
    
    test_sentences = [
        "áŠ¢á‰µá‹®áŒµá‹« á‹á‰¥ áˆ€áŒˆáˆ­ áŠá‰½",  # Ethiopia is a beautiful country
        "á‰¡áŠ“ á‹­á‹ˆá‹³áˆ‰",             # They like coffee  
        "áŠ áˆ›áˆ­áŠ› á‰‹áŠ•á‰‹",            # Amharic language
        "áˆ°áˆ‹áˆ áŠ¥áŠ•á‹´á‰µ áŠáˆ½"          # Hello, how are you (fem)
    ]
    
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\n{i}. Testing sentence: {sentence}")
        
        # Convert to bytes
        sentence_bytes = sentence.encode('utf-8')
        input_ids = torch.tensor([[b for b in sentence_bytes]], dtype=torch.long)
        
        try:
            with torch.no_grad():
                # Full forward pass
                logits, boundary_probs = model.forward(input_ids)
                
                print(f"   ğŸ“ Input length: {len(sentence_bytes)} bytes")
                print(f"   ğŸ“ Output shape: {logits.shape}")
                print(f"   ğŸ”„ Boundary shape: {boundary_probs.shape}")
                
                # Count predicted morpheme boundaries
                boundaries = (boundary_probs[0] > 0.5).sum().item()
                print(f"   ğŸ§© Predicted morphemes: {boundaries}")
                
                # Test generation capability
                generated = model.generate(
                    input_ids,
                    max_length=len(sentence_bytes) + 10,
                    temperature=0.7,
                    top_k=30
                )
                
                # Convert back to text
                generated_bytes = generated[0].cpu().numpy()
                
                # Filter and decode
                filtered_bytes = []
                for byte_val in generated_bytes:
                    if 32 <= byte_val <= 126 or byte_val >= 128:
                        filtered_bytes.append(byte_val)
                
                try:
                    generated_text = bytes(filtered_bytes).decode('utf-8', errors='ignore')
                    print(f"   ğŸ¤– Generated: {generated_text}")
                    
                    # Check if it contains original sentence
                    if sentence in generated_text:
                        continuation = generated_text.replace(sentence, "").strip()
                        print(f"   â• Continuation: '{continuation}'")
                    
                    # Amharic character analysis
                    amharic_chars = sum(1 for c in generated_text if 0x1200 <= ord(c) <= 0x137F)
                    total_chars = len(generated_text)
                    if total_chars > 0:
                        ratio = amharic_chars / total_chars
                        print(f"   ğŸ“Š Amharic ratio: {ratio:.1%} ({amharic_chars}/{total_chars})")
                    
                except Exception as decode_error:
                    print(f"   âš ï¸ Decode error: {decode_error}")
                    
        except Exception as e:
            print(f"   âŒ Processing error: {e}")

def test_pattern_recognition():
    """Test if the model recognizes Amharic morphological patterns"""
    
    print(f"\nğŸ§  Testing Morphological Pattern Recognition")
    print("=" * 50)
    
    # Load model  
    model_path = "outputs/compact/final_model.pt"
    model = AmharicHNet(d_model=256, n_encoder_layers=2, n_decoder_layers=2, n_main_layers=4, n_heads=4, vocab_size=256)
    checkpoint = torch.load(model_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # Check what patterns the chunker knows
    chunker = model.chunker
    
    print("ğŸ“š Built-in Morphological Knowledge:")
    print(f"   Verb prefixes: {chunker.verb_prefixes}")
    print(f"   Verb suffixes: {chunker.verb_suffixes}")
    print(f"   Noun prefixes: {chunker.noun_prefixes}")
    print(f"   Noun suffixes: {chunker.noun_suffixes}")
    
    print(f"\nğŸ¯ This H-Net architecture has {len(chunker.verb_prefixes + chunker.verb_suffixes + chunker.noun_prefixes + chunker.noun_suffixes)} built-in Amharic morphological patterns!")
    
    return True

if __name__ == "__main__":
    print("ğŸ‡ªğŸ‡¹ H-Net Amharic Architecture Component Testing")
    print("=" * 60)
    
    # Test morphological patterns first
    test_pattern_recognition()
    
    # Test morpheme chunker
    model = test_morpheme_chunker()
    
    # Test hierarchical processing
    test_hierarchical_processing(model)
    
    print(f"\nğŸ‰ H-Net Component Testing Complete!")
    print("ğŸ¯ This shows how well our morpheme-aware architecture is working")