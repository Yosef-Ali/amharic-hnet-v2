# Amharic H-Net v2

ğŸ‡ªğŸ‡¹ **Advanced Hierarchical Neural Network for Amharic Language Modeling**

Amharic H-Net v2 is a state-of-the-art language model specifically designed for Amharic text generation and understanding. It uses hierarchical neural networks with dynamic chunking to handle the morphological complexity of Amharic without traditional tokenization.

## âœ¨ Features

- **ğŸ”„ Dynamic Chunking**: No tokenization needed - works directly with byte sequences
- **ğŸ—ï¸ Hierarchical Architecture**: Multi-level processing for better morpheme understanding
- **ğŸ›¡ï¸ Cultural Safety**: Advanced guardrails with dialect-aware cultural protection
- **ğŸ“Š Morpheme-Aware**: Deep Amharic morphological analysis with prefix/suffix detection
- **ğŸŒ Multi-Dialect Support**: Handles Addis Ababa, Gojjam, and Eritrean variants
- **âš¡ Efficient Training**: Optimized for both CPU and GPU training
- **ğŸ¯ Compression Control**: Configurable compression ratios for different use cases
- **ğŸ”¤ Space-Free Processing**: Handles authentic Amharic text without artificial spaces
- **ğŸ§ª Comprehensive Testing**: Built-in morpheme and cultural safety validation

## ğŸš€ Quick Start

### Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd amharic-hnet-v2
   ```

2. **Run the setup script**:
   ```bash
   ./setup.sh
   ```

3. **Activate the virtual environment**:
   ```bash
   source venv/bin/activate
   ```

### Basic Usage

1. **Test the installation**:
   ```bash
   python main.py test
   ```

2. **Train a model** (using sample data):
   ```bash
   python main.py train --config configs/config.yaml --data-dir data/processed
   ```

3. **Generate text**:
   ```bash
   python main.py generate --model-path outputs/checkpoint_best.pt --prompt "áŠ áˆ›áˆ­áŠ›"
   ```

4. **Evaluate the model**:
   ```bash
   python main.py evaluate --model-path outputs/checkpoint_best.pt
   ```

## ğŸ“ Project Structure

```
amharic-hnet-v2/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ models/                   # H-Net model implementation
â”‚   â”‚   â””â”€â”€ hnet_amharic.py      # Main model architecture
â”‚   â”œâ”€â”€ preprocessing/           # Text preprocessing utilities
â”‚   â”‚   â””â”€â”€ prepare_amharic.py   # Amharic-specific preprocessing
â”‚   â”œâ”€â”€ safety/                  # Cultural safety components
â”‚   â”‚   â””â”€â”€ cultural_guardrails.py # Cultural safety checks
â”‚   â”œâ”€â”€ training/                # Training infrastructure
â”‚   â”‚   â”œâ”€â”€ train.py            # Training script
â”‚   â”‚   â””â”€â”€ data_loader.py      # Data loading utilities
â”‚   â””â”€â”€ evaluation/              # Evaluation tools
â”‚       â””â”€â”€ evaluate.py         # Comprehensive evaluation
â”œâ”€â”€ configs/                     # Configuration files
â”‚   â””â”€â”€ config.yaml             # Main configuration
â”œâ”€â”€ data/                       # Data directories
â”‚   â”œâ”€â”€ raw/                    # Raw text files
â”‚   â”œâ”€â”€ processed/              # Processed training data
â”‚   â””â”€â”€ morpheme_annotated/     # Morpheme annotations
â”œâ”€â”€ outputs/                    # Model checkpoints and logs
â”œâ”€â”€ evaluation_results/         # Evaluation outputs
â”œâ”€â”€ main.py                     # CLI entry point
â”œâ”€â”€ setup.sh                    # Setup script
â””â”€â”€ requirements.txt            # Python dependencies
```

## ğŸ”§ Configuration

The main configuration is in `configs/config.yaml`. Key parameters:

```yaml
model:
  d_model: 768              # Hidden dimension
  compression_ratio: 4.5    # Target compression ratio
  n_main_layers: 12        # Number of transformer layers

training:
  batch_size: 16           # Training batch size
  learning_rate: 1e-4      # Learning rate
  num_epochs: 10           # Training epochs

cultural_safety:
  enable_runtime_checking: true  # Enable cultural safety
  protect_sacred_terms: true     # Protect sacred terms
```

## ğŸ“Š Model Architecture

### H-Net Components

1. **Dynamic Chunker**: Identifies morpheme boundaries without tokenization
2. **Hierarchical Encoder**: Processes text at byte and chunk levels
3. **Main Transformer**: Standard transformer layers for language modeling
4. **Cultural Safety Layer**: Monitors and filters inappropriate content

### Key Innovations

- **Byte-level Processing**: Works directly with UTF-8 bytes
- **Morpheme Awareness**: Understands Amharic morphological patterns
- **Cultural Context**: Respects Amharic cultural and religious sensitivities
- **Dynamic Compression**: Adapts chunking to text complexity

## ğŸ›¡ï¸ Cultural Safety

The model includes comprehensive cultural safety features:

- **Sacred Terms Protection**: Prevents inappropriate use of religious/cultural terms
- **Context-Aware Filtering**: Understands cultural context and sensitivities
- **Real-time Monitoring**: Checks generated content for cultural appropriateness
- **Violation Reporting**: Detailed feedback on cultural safety issues

### Protected Terms Examples
- `á‰¡áŠ“` (Coffee ceremony) - Protected as cultural ritual
- `áˆ˜áˆµá‰€áˆ` (Cross) - Protected as religious symbol  
- `á‰€á‹³áˆ›á‹Š` (Emperor) - Protected as historical title

## ğŸ“ˆ Training

### Data Preparation

1. **Prepare your data**:
   ```bash
   python main.py preprocess --input data/raw/your_corpus.txt --output data/processed
   ```

2. **Train the model**:
   ```bash
   python main.py train --config configs/config.yaml --data-dir data/processed
   ```

### Training Features

- **Mixed Precision**: Automatic mixed precision for faster training
- **Gradient Clipping**: Prevents gradient explosion
- **Learning Rate Scheduling**: Cosine annealing with warmup
- **Early Stopping**: Prevents overfitting
- **Tensorboard Logging**: Real-time training monitoring

## ğŸ¯ Evaluation

The evaluation suite includes:

- **Perplexity**: Language modeling performance
- **Compression Ratio**: Dynamic chunking effectiveness  
- **Cultural Safety**: Safety violation detection
- **Amharic Quality**: Language-specific quality metrics

### Running Evaluation

```bash
python main.py evaluate \
  --model-path outputs/checkpoint_best.pt \
  --test-data data/processed \
  --output-dir evaluation_results
```

## ğŸ”¤ Text Generation

### Basic Generation

```bash
python main.py generate \
  --model-path outputs/checkpoint_best.pt \
  --prompt "áŠ áˆ›áˆ­áŠ› á‰‹áŠ•á‰‹" \
  --max-length 200 \
  --temperature 0.8
```

### Advanced Parameters

- `--temperature`: Controls randomness (0.1-2.0)
- `--top-k`: Top-k sampling parameter
- `--top-p`: Nucleus sampling parameter
- `--num-samples`: Number of samples to generate

## ğŸ” Development

### Adding New Features

1. **Model Components**: Add to `src/models/`
2. **Preprocessing**: Extend `src/preprocessing/`
3. **Safety Rules**: Update `src/safety/cultural_guardrails.py`
4. **Training Logic**: Modify `src/training/`

### Testing

```bash
# Test installation
python main.py test

# Test Amharic morpheme processing
python tests/test_morphemes.py

# Test preprocessing
python main.py preprocess --create-sample --output data/test

# Test with small config
python main.py train --config configs/config.yaml --data-dir data/test
```

## ğŸ“‹ Requirements

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.8+ (for GPU training)
- 8GB+ RAM (16GB+ recommended)
- 2GB+ disk space

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Ethiopian AI community for cultural guidance
- Amharic language experts for linguistic insights
- Open source communities for foundational tools

## ğŸ“ Support

- **Issues**: GitHub Issues
- **Discussions**: GitHub Discussions
- **Documentation**: This README and inline code comments

## ğŸ—ºï¸ Roadmap

- [ ] Multi-GPU distributed training
- [ ] ONNX model export
- [ ] REST API server
- [ ] Web interface
- [ ] Integration with other Ethiopian languages
- [ ] Mobile deployment support

---

**Made with â¤ï¸ for the Amharic language community**