# Base training configuration for Amharic H-Net v2
experiment:
  name: "amharic-hnet-base"
  description: "Base training configuration for Amharic H-Net v2"
  tags: ["amharic", "hnet", "base", "morphology"]
  project_name: "amharic-hnet-v2"

model:
  type: "hnet_amharic"
  hidden_size: 768
  num_layers: 12
  num_heads: 12
  intermediate_size: 3072
  vocab_size: 32000
  max_position_embeddings: 512
  dropout_prob: 0.1
  layer_norm_eps: 1e-12

training:
  # Training parameters
  num_epochs: 10
  batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 1000
  max_grad_norm: 1.0
  
  # Optimization
  optimizer: "adamw"
  scheduler: "linear_warmup"
  
  # Mixed precision
  use_fp16: true
  fp16_opt_level: "O1"

data:
  # Data processing
  max_length: 512
  morpheme_masking_prob: 0.15
  whole_word_masking: true
  
  # Data paths
  train_file: "data/processed/train.jsonl"
  val_file: "data/processed/val.jsonl"
  vocab_file: "data/processed/vocab.txt"
  
  # Data loading
  num_workers: 4
  pin_memory: true
  drop_last: true

logging:
  # Experiment tracking
  use_wandb: true
  use_tensorboard: true
  
  # Logging frequency
  log_steps: 100
  eval_steps: 500
  save_steps: 1000
  
  # Output directories
  output_dir: "./outputs/base_training"
  logging_dir: "./outputs/base_training/logs"
  tensorboard_dir: "./outputs/tensorboard/base_training"

evaluation:
  # Evaluation settings
  eval_batch_size: 16
  eval_accumulation_steps: 1
  
  # Metrics to compute
  compute_perplexity: true
  compute_morpheme_accuracy: true
  compute_bleu: false
  
  # Text generation for evaluation
  generate_samples: true
  num_samples: 10
  max_generation_length: 100

hardware:
  # Device settings
  device: "auto"  # auto, cpu, cuda
  gpu_ids: [0]
  distributed: false
  
  # Memory optimization
  gradient_checkpointing: false
  dataloader_pin_memory: true